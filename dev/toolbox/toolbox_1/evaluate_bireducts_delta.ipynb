{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "\n",
    "import attr\n",
    "import config\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils\n",
    "import tqdm.notebook\n",
    "\n",
    "import skrough\n",
    "from skrough.metrics.gini_impurity import gini_impurity\n",
    "from skrough.utils.group_index import (\n",
    "    compute_dec_distribution,\n",
    "    compute_homogeneity,\n",
    "    split_groups,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(config.DATA_DIR)\n",
    "TMP_DIR = pathlib.Path(config.TMP_DIR)\n",
    "SEP = \";\"\n",
    "\n",
    "EVAL_SETUP_1 = {\n",
    "    \"data_filepath\": DATA_DIR / \"train_utf.csv\",\n",
    "    \"bireducts_filepath\": TMP_DIR / \"bireducts_10_10000.json\",\n",
    "}\n",
    "\n",
    "EVAL_SETUP_2 = {\n",
    "    \"data_filepath\": DATA_DIR / \"train_utf.csv\",\n",
    "    \"bireducts_filepath\": TMP_DIR / \"bireducts_20_10000.json\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __prepare_values(values):\n",
    "    factorized_values, uniques = pd.factorize(values)\n",
    "    uniques = len(uniques)\n",
    "    return factorized_values, uniques\n",
    "\n",
    "\n",
    "def prepare_df(filepath=None, df=None):\n",
    "    if df is None:\n",
    "        df = pd.read_csv(filepath, sep=SEP)\n",
    "    df_dec = df.pop(\"target\")\n",
    "    df = df.astype(\"category\")\n",
    "    df = df.apply(lambda x: x.cat.codes)\n",
    "\n",
    "    x = df\n",
    "    y = df_dec\n",
    "    x, y = sklearn.utils.check_X_y(x, y, multi_output=False)\n",
    "    data = np.apply_along_axis(__prepare_values, 0, x)\n",
    "    x = np.vstack(data[0]).T\n",
    "    x_count_distinct = data[1]\n",
    "    y, y_count_distinct = __prepare_values(y)\n",
    "\n",
    "    return x, x_count_distinct, y, y_count_distinct\n",
    "\n",
    "\n",
    "def _compute_chaos_score(group_index, n_groups, xx, yy, yy_count_distinct):\n",
    "    distribution = compute_dec_distribution(\n",
    "        group_index, n_groups, yy, yy_count_distinct\n",
    "    )\n",
    "    return gini_impurity(distribution, len(xx))\n",
    "\n",
    "\n",
    "def get_chaos_score(xx, xx_count_distinct, yy, yy_count_distinct, attrs):\n",
    "    group_index = np.zeros(len(xx), dtype=np.int_)\n",
    "    n_groups = 1\n",
    "    for attr in attrs:\n",
    "        group_index, n_groups = split_groups(\n",
    "            group_index,\n",
    "            n_groups,\n",
    "            xx[:, attr],\n",
    "            xx_count_distinct[attr],\n",
    "            compress_group_index=True,\n",
    "        )\n",
    "    result = _compute_chaos_score(group_index, n_groups, xx, yy, yy_count_distinct)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_results(eval_setup, local_scope=False, sep=SEP):\n",
    "    df_columns = pd.read_csv(eval_setup[\"data_filepath\"], sep=sep, nrows=0).columns[:-1]\n",
    "\n",
    "    xx, xx_count_distinct, yy, yy_count_distinct = prepare_df(\n",
    "        eval_setup[\"data_filepath\"]\n",
    "    )\n",
    "    results = {}\n",
    "    filepath = eval_setup[\"bireducts_filepath\"]\n",
    "    with filepath.open(\"r\") as f:\n",
    "        bireducts = json.load(f)\n",
    "    counts = np.zeros(xx.shape[1])\n",
    "    scores = np.zeros(xx.shape[1])\n",
    "    scores_2 = np.zeros(xx.shape[1])\n",
    "    for bireduct in tqdm.notebook.tqdm(bireducts):\n",
    "        bireduct_objects = bireduct[\"objects\"]\n",
    "        bireduct_all_attrs = set(bireduct[\"attributes\"])\n",
    "        xxx = xx\n",
    "        yyy = yy\n",
    "        if local_scope:\n",
    "            xxx = xxx[bireduct_objects]\n",
    "            yyy = yyy[bireduct_objects]\n",
    "        starting_chaos_score = get_chaos_score(\n",
    "            xxx, xx_count_distinct, yyy, yy_count_distinct, bireduct_all_attrs\n",
    "        )\n",
    "        counts[bireduct[\"attributes\"]] += 1\n",
    "        for attr in bireduct[\"attributes\"]:\n",
    "            attrs_to_check = bireduct_all_attrs.difference([attr])\n",
    "            current_chaos_score = get_chaos_score(\n",
    "                xxx, xx_count_distinct, yyy, yy_count_distinct, attrs_to_check\n",
    "            )\n",
    "            score_val = current_chaos_score - starting_chaos_score\n",
    "            scores[attr] += score_val\n",
    "            scores_2[attr] += score_val * len(bireduct_objects) / xx.shape[0]\n",
    "    avg = np.divide(scores, counts, out=np.zeros_like(scores), where=counts > 0)\n",
    "    avg_2 = np.divide(scores_2, counts, out=np.zeros_like(scores_2), where=counts > 0)\n",
    "    results[filepath.name] = pd.DataFrame(\n",
    "        {\n",
    "            \"column\": df_columns,\n",
    "            \"score\": scores,\n",
    "            \"score2\": scores_2,\n",
    "            \"count\": counts,\n",
    "            \"avg\": avg,\n",
    "            \"avg2\": avg_2,\n",
    "        }\n",
    "    ).sort_values([\"score\"], ascending=False)\n",
    "    del bireducts\n",
    "    return results\n",
    "\n",
    "\n",
    "def do_eval(eval_setup, scope):\n",
    "    assert scope in (\"global\", \"local\")\n",
    "    results_dir = eval_setup[\"bireducts_filepath\"].parent\n",
    "    results_filename = f\"{eval_setup['bireducts_filepath'].stem}_eval_scope_{scope}.pkl\"\n",
    "    results = get_results(eval_setup, local_scope=True if scope == \"local\" else False)\n",
    "    with (results_dir / results_filename).open(\"wb\") as f:\n",
    "        pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_eval(EVAL_SETUP_1, 'local')\n",
    "# do_eval(EVAL_SETUP_1, 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb353cb8b9a4ebafc945efb5c6e15e592d37ff0fcf51b748ec6c5769c1eaa51b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
