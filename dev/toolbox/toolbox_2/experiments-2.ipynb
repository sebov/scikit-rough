{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "import attr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import xgboost as xgb\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import auc, precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from skrough.base import Bireduct\n",
    "from skrough.bireducts.dynamically_adapted_approximate_bireduct import (\n",
    "    DynamicallyAdaptedApproximateBireduct,\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(config.DATA_DIR)\n",
    "TMP_DIR = pathlib.Path(config.TMP_DIR)\n",
    "SEP = \",\"\n",
    "DISCRETIZED_SEP = \";\"\n",
    "N_JOBS = 7\n",
    "\n",
    "# DISCRETIZED_FILEPATH_IN = DATA_DIR / 'toolbox_tabular_data_annonymized_discretized_uniform.csv'\n",
    "DISCRETIZED_FILEPATH_IN = (\n",
    "    DATA_DIR / \"toolbox_tabular_data_annonymized_discretized_quantile.csv\"\n",
    ")\n",
    "# DISCRETIZED_FILEPATH_IN = DATA_DIR / 'toolbox_tabular_data_annonymized_discretized_kmeans.csv'\n",
    "FILEPATH_IN = DATA_DIR / \"toolbox_tabular_data_annonymized.csv\"\n",
    "\n",
    "EMBEDDINGS_FILEPATH_IN = DATA_DIR / \"process_embeddings_d50.csv\"\n",
    "\n",
    "XGBOOST_EXPERIMENT_DIR = \"paper_xgboost_hyper_native_2_\"\n",
    "XGBOOST_FILENAME_OUT_TEMPLATE = (\n",
    "    f\"{XGBOOST_EXPERIMENT_DIR}/\"\n",
    "    \"xgboost_native\"\n",
    "    f\"_dataset_{pathlib.Path(FILEPATH_IN).stem}\"\n",
    "    \"_leave_one_process_out_k_{k}\"\n",
    "    \"_num_boost_round_{num_boost_round}\"\n",
    "    \"_learning_rate_{learning_rate}\"\n",
    "    \"_max_depth_{max_depth}\"\n",
    "    \"_base_score_{base_score}\"\n",
    "    \".csv\"\n",
    ")\n",
    "\n",
    "XGBOOST_FILENAME_OUT_TEMPLATE_SAME_CATEGORY = (\n",
    "    f\"{XGBOOST_EXPERIMENT_DIR}/\"\n",
    "    \"xgboost_native\"\n",
    "    f\"_dataset_{pathlib.Path(FILEPATH_IN).stem}\"\n",
    "    \"_leave_one_process_out_same_category\"\n",
    "    \"_num_boost_round_{num_boost_round}\"\n",
    "    \"_learning_rate_{learning_rate}\"\n",
    "    \"_max_depth_{max_depth}\"\n",
    "    \"_base_score_{base_score}\"\n",
    "    \".csv\"\n",
    ")\n",
    "\n",
    "\n",
    "TREE_EXPERIMENT_DIR = \"paper_tree_2_\"\n",
    "TREE_FILENAME_OUT_TEMPLATE = (\n",
    "    f\"{TREE_EXPERIMENT_DIR}/\"\n",
    "    \"decision_tree\"\n",
    "    f\"_dataset_{pathlib.Path(FILEPATH_IN).stem}\"\n",
    "    \"_leave_one_process_out_k_{k}\"\n",
    "    \"_min_impurity_decrease_{min_impurity_decrease}\"\n",
    "    \"_max_depth_{max_depth}\"\n",
    "    \".csv\"\n",
    ")\n",
    "TREE_FILENAME_OUT_TEMPLATE_SAME_CATEGORY = (\n",
    "    f\"{TREE_EXPERIMENT_DIR}/\"\n",
    "    \"decision_tree\"\n",
    "    f\"_dataset_{pathlib.Path(FILEPATH_IN).stem}\"\n",
    "    \"_leave_one_process_out_same_category\"\n",
    "    \"_min_impurity_decrease_{min_impurity_decrease}\"\n",
    "    \"_max_depth_{max_depth}\"\n",
    "    \".csv\"\n",
    ")\n",
    "\n",
    "\n",
    "BIREDUCTS_EXPERIMENT_DIR = \"paper_bireducts_2_\"\n",
    "BIREDUCTS_FILENAME_OUT_TEMPLATE = (\n",
    "    f\"{BIREDUCTS_EXPERIMENT_DIR}/\"\n",
    "    \"bireducts\"\n",
    "    f\"_dataset_{pathlib.Path(DISCRETIZED_FILEPATH_IN).stem}\"\n",
    "    \"_leave_one_process_out_k_{k}\"\n",
    "    \"_n_bireducts_{n_bireducts}\"\n",
    "    \"_candidate_n_attrs_{candidate_n_attrs}\"\n",
    "    \"_allowed_randomness_{allowed_randomness}\"\n",
    "    \"_max_n_attrs_{max_n_attrs}\"\n",
    "    \"_iteration_{iteration}\"\n",
    "    \".csv\"\n",
    ")\n",
    "BIREDUCTS_FILENAME_OUT_TEMPLATE_SAME_CATEGORY = (\n",
    "    f\"{BIREDUCTS_EXPERIMENT_DIR}/\"\n",
    "    \"bireducts\"\n",
    "    f\"_dataset_{pathlib.Path(DISCRETIZED_FILEPATH_IN).stem}\"\n",
    "    \"_leave_one_process_out_same_category\"\n",
    "    \"_n_bireducts_{n_bireducts}\"\n",
    "    \"_candidate_n_attrs_{candidate_n_attrs}\"\n",
    "    \"_allowed_randomness_{allowed_randomness}\"\n",
    "    \"_max_n_attrs_{max_n_attrs}\"\n",
    "    \"_iteration_{iteration}\"\n",
    "    \".csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=TMP_DIR / \"b.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILEPATH_IN, sep=SEP)\n",
    "process_ids = df.pop(\"process_ids\")\n",
    "df_dec = df.pop(\"target\").astype(\"category\").cat.codes\n",
    "df_dec = 1 - df_dec\n",
    "\n",
    "discretized_df = pd.read_csv(DISCRETIZED_FILEPATH_IN, sep=DISCRETIZED_SEP)\n",
    "discretized_process_ids = discretized_df.pop(\"process_ids\")\n",
    "discretized_df = discretized_df.astype(\"category\")\n",
    "discretized_df = discretized_df.apply(lambda x: x.cat.codes)\n",
    "discretized_df_dec = discretized_df.pop(\"target\")\n",
    "discretized_df_dec = 1 - discretized_df_dec\n",
    "\n",
    "embeddings = pd.read_csv(EMBEDDINGS_FILEPATH_IN)\n",
    "process_category = embeddings[[\"process_ids\", \"process_category\"]]\n",
    "embeddings.drop([\"process_category\"], axis=1, inplace=True)\n",
    "embeddings.set_index(\"process_ids\", inplace=True)\n",
    "emb_nbrs = NearestNeighbors().fit(embeddings)\n",
    "\n",
    "assert (process_ids == discretized_process_ids).all()\n",
    "assert (df_dec == discretized_df_dec).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06968954656801479"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_dec) / len(df_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06968954656801479"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(discretized_df_dec) / len(discretized_df_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_train(train, test, process_ids, embeddings, emb_nbrs, k):\n",
    "    # search for k+1 neighbors because the one we search for is obviously the best match (thus +1)\n",
    "    test_process_id = process_ids.loc[test.index[0]]\n",
    "    nbrs = emb_nbrs.kneighbors(\n",
    "        [embeddings.loc[test_process_id]], n_neighbors=k + 1, return_distance=False\n",
    "    )[0]\n",
    "    return train[process_ids.loc[train.index].isin(embeddings.index[nbrs])]\n",
    "\n",
    "\n",
    "def fit_predict_xgboost_native(train, test, df_dec, **fit_predict_params):\n",
    "    params = dict(\n",
    "        fit_predict_params, objective=\"binary:logistic\", eval_metric=\"logloss\"\n",
    "    )\n",
    "    num_boost_round = params.pop(\"num_boost_round\")\n",
    "    dtrain = xgb.DMatrix(train.values, label=df_dec.loc[train.index])\n",
    "    dtest = xgb.DMatrix(test.values)\n",
    "    cl = xgb.train(params, dtrain, num_boost_round=num_boost_round)\n",
    "    logging.info(f\"num_boost_rounds == {num_boost_round}\")\n",
    "    scores = cl.predict(dtest)\n",
    "    return (\n",
    "        pd.Series(scores, index=test.index),\n",
    "        pd.Series(1, index=test.index),\n",
    "        pd.Series(1, index=test.index),\n",
    "    )\n",
    "\n",
    "\n",
    "def fit_predict_decision_tree(train, test, df_dec, **fit_predict_params):\n",
    "    cl = tree.DecisionTreeClassifier(**fit_predict_params)\n",
    "    cl = cl.fit(train.values, df_dec.loc[train.index])\n",
    "    scores = cl.predict_proba(test)[:, 1]\n",
    "    return (\n",
    "        pd.Series(scores, index=test.index),\n",
    "        pd.Series(1, index=test.index),\n",
    "        pd.Series(1, index=test.index),\n",
    "    )\n",
    "\n",
    "\n",
    "def get_bireducts(\n",
    "    train,\n",
    "    df_dec,\n",
    "    n_bireducts,\n",
    "    n_of_probes,\n",
    "    allowed_randomness,\n",
    "    candidate_n_attrs,\n",
    "    max_n_attrs,\n",
    "):\n",
    "    ghr = DynamicallyAdaptedApproximateBireduct(\n",
    "        n_of_probes=n_of_probes,\n",
    "        allowed_randomness=allowed_randomness,\n",
    "        candidate_n_attrs=candidate_n_attrs,\n",
    "        max_n_attrs=max_n_attrs,\n",
    "    )\n",
    "    ghr.fit(train, df_dec.loc[train.index], check_data_consistency=False)\n",
    "    bireducts = Parallel(n_jobs=N_JOBS)(\n",
    "        delayed(ghr.get_bireduct)() for i in range(n_bireducts)\n",
    "    )\n",
    "    return bireducts\n",
    "\n",
    "\n",
    "def predict(train, test, bireduct, df_dec):\n",
    "    objs = bireduct.objects\n",
    "    cols = bireduct.attributes\n",
    "    bireduct_train = train.iloc[objs, cols]\n",
    "    bireduct_train = bireduct_train.drop_duplicates()\n",
    "    nn = NearestNeighbors().fit(bireduct_train)\n",
    "    dist, neighbors = nn.kneighbors(\n",
    "        test.iloc[:, cols], n_neighbors=1, return_distance=True\n",
    "    )\n",
    "    result = [\n",
    "        df_dec.loc[bireduct_train.index[n[0]]] if d[0] == 0 else np.nan\n",
    "        for d, n in zip(dist, neighbors)\n",
    "    ]\n",
    "    return result\n",
    "\n",
    "\n",
    "def fit_predict_bireducts(train, test, df_dec, **fit_predict_params):\n",
    "    params = dict(fit_predict_params, n_of_probes=100)\n",
    "    bireducts = get_bireducts(train, df_dec, **params)\n",
    "    scores = np.zeros(len(test))\n",
    "    counts = np.zeros(len(test))\n",
    "    counts2 = np.zeros(len(test))\n",
    "    for bireduct in bireducts:\n",
    "        for i, dec in enumerate(predict(train, test, bireduct, df_dec)):\n",
    "            if not np.isnan(dec):\n",
    "                scores[i] += int(dec)\n",
    "                counts[i] += 1\n",
    "                counts2[i] += len(bireduct.objects) / len(train)\n",
    "    return (\n",
    "        pd.Series(scores, index=test.index),\n",
    "        pd.Series(counts, index=test.index),\n",
    "        pd.Series(counts2, index=test.index),\n",
    "    )\n",
    "\n",
    "\n",
    "def do_leave_one_process_out_k(\n",
    "    fit_predict_fun, df, df_dec, process_ids, k, **fit_predict_params\n",
    "):\n",
    "    logging.info({\"filename\": FILEPATH_IN})\n",
    "    logging.info({\"k\": k})\n",
    "    logging.info(fit_predict_params)\n",
    "    scores = pd.Series(dtype=\"float\")\n",
    "    counts = pd.Series(dtype=\"float\")\n",
    "    counts2 = pd.Series(dtype=\"float\")\n",
    "    i = 0\n",
    "    for train_index, test_index in tqdm.tqdm(\n",
    "        LeaveOneGroupOut().split(df, df_dec, groups=process_ids)\n",
    "    ):\n",
    "        i += 1\n",
    "        logging.info(str(i))\n",
    "        train = df.iloc[train_index]\n",
    "        test = df.iloc[test_index]\n",
    "        if k is not None:\n",
    "            train = refine_train(train, test, process_ids, embeddings, emb_nbrs, k)\n",
    "        s, c, c2 = fit_predict_fun(train, test, df_dec, **fit_predict_params)\n",
    "        scores = scores.append(s)\n",
    "        counts = counts.append(c)\n",
    "        counts2 = counts2.append(c2)\n",
    "    return scores.sort_index(), counts.sort_index(), counts2.sort_index()\n",
    "\n",
    "\n",
    "def do_leave_one_process_out_same_category(\n",
    "    fit_predict_fun, df, df_dec, process_ids, process_category, **fit_predict_params\n",
    "):\n",
    "    logging.info(\"same category\")\n",
    "    logging.info(fit_predict_params)\n",
    "    scores = pd.Series(dtype=\"float\")\n",
    "    counts = pd.Series(dtype=\"float\")\n",
    "    counts2 = pd.Series(dtype=\"float\")\n",
    "    i = 0\n",
    "    for leave_one_out_process_id in tqdm.tqdm(process_category[\"process_ids\"].unique()):\n",
    "        i += 1\n",
    "        logging.info(str(i))\n",
    "        test = df[process_ids == leave_one_out_process_id]\n",
    "        test_category = process_category[\n",
    "            process_category[\"process_ids\"] == leave_one_out_process_id\n",
    "        ][\"process_category\"].iloc[0]\n",
    "        test_category_process_ids = process_category[\n",
    "            process_category[\"process_category\"] == test_category\n",
    "        ][\"process_ids\"]\n",
    "        train = df[\n",
    "            process_ids.isin(test_category_process_ids)\n",
    "            & (process_ids != leave_one_out_process_id)\n",
    "        ]\n",
    "        s, c, c2 = fit_predict_fun(train, test, df_dec, **fit_predict_params)\n",
    "        scores = scores.append(s)\n",
    "        counts = counts.append(c)\n",
    "        counts2 = counts2.append(c2)\n",
    "    return scores.sort_index(), counts.sort_index(), counts2.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = do_leave_one_process_out_k(fit_predict_decision_tree,\n",
    "#                                df, df_dec, process_ids, k=40,\n",
    "#                                min_impurity_decrease=0.001,\n",
    "#                                max_depth=3,\n",
    "#                               )\n",
    "# roc_auc_score(df_dec, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.94s/it]"
     ]
    }
   ],
   "source": [
    "k_values = list(range(10, 202, 10)) + [None]\n",
    "learning_rate_values = [0.001]\n",
    "max_depth_values = [3]\n",
    "num_boost_round_values = [1000]\n",
    "more_params_values = [{\"base_score\": 0.0696}]\n",
    "\n",
    "# k_values = [40]\n",
    "# learning_rate_values = [0.1, 0.01, 0.001]\n",
    "# max_depth_values = [2, 3, 4, 5, 10]\n",
    "# num_boost_round_values = [1000]\n",
    "# more_params_values = [{'base_score': 0.0696}, {'base_score': 0.5}]\n",
    "\n",
    "for more_params in more_params_values:\n",
    "    for k in k_values:\n",
    "        for num_boost_round in num_boost_round_values:\n",
    "            for learning_rate in learning_rate_values:\n",
    "                for max_depth in max_depth_values:\n",
    "                    x = do_leave_one_process_out_k(\n",
    "                        fit_predict_xgboost_native,\n",
    "                        df,\n",
    "                        df_dec,\n",
    "                        process_ids,\n",
    "                        k=k,\n",
    "                        num_boost_round=num_boost_round,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_depth=max_depth,\n",
    "                        **more_params,\n",
    "                    )\n",
    "                    pd.DataFrame(\n",
    "                        {\"scores\": x[0], \"counts\": x[1], \"counts2\": x[2]}\n",
    "                    ).to_csv(\n",
    "                        TMP_DIR\n",
    "                        / XGBOOST_FILENAME_OUT_TEMPLATE.format(\n",
    "                            k=k,\n",
    "                            num_boost_round=num_boost_round,\n",
    "                            learning_rate=learning_rate,\n",
    "                            max_depth=max_depth,\n",
    "                            **more_params,\n",
    "                        ),\n",
    "                        sep=\";\",\n",
    "                        index=False,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 50/202 [07:05<23:49,  9.40s/it]"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "max_depth = 3\n",
    "num_boost_round = 1000\n",
    "more_params = {\"base_score\": 0.0696}\n",
    "\n",
    "x = do_leave_one_process_out_same_category(\n",
    "    fit_predict_xgboost_native,\n",
    "    df,\n",
    "    df_dec,\n",
    "    process_ids,\n",
    "    process_category,\n",
    "    num_boost_round=num_boost_round,\n",
    "    learning_rate=learning_rate,\n",
    "    max_depth=max_depth,\n",
    "    **more_params,\n",
    ")\n",
    "pd.DataFrame({\"scores\": x[0], \"counts\": x[1], \"counts2\": x[2]}).to_csv(\n",
    "    TMP_DIR\n",
    "    / XGBOOST_FILENAME_OUT_TEMPLATE_SAME_CATEGORY.format(\n",
    "        num_boost_round=num_boost_round,\n",
    "        learning_rate=learning_rate,\n",
    "        max_depth=max_depth,\n",
    "        **more_params,\n",
    "    ),\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = pd.read_csv('./tmp/to_remove/xgboost_leave_one_process_out_k_40_annonymized_discretized_quantile_num_boost_round_1_learning_rate_0.01_max_depth_3.csv', sep=';')\n",
    "# roc_auc_score(df_dec, q['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = do_leave_one_process_out_k(fit_predict_xgboost_native,\n",
    "#                                            df, df_dec, process_ids, k=40,\n",
    "#                                            num_boost_round=1,\n",
    "#                                            learning_rate=0.01,\n",
    "#                                            max_depth=3)\n",
    "# roc_auc_score(df_dec, x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "202it [00:11, 17.55it/s]\n",
      "202it [00:12, 15.69it/s]\n",
      "202it [00:13, 15.41it/s]\n",
      "202it [00:11, 18.12it/s]\n",
      "202it [00:12, 16.74it/s]\n",
      "202it [00:12, 16.27it/s]\n",
      "202it [00:13, 15.52it/s]\n",
      "202it [00:16, 12.24it/s]\n",
      "202it [00:17, 11.40it/s]\n",
      "202it [00:13, 15.00it/s]\n",
      "202it [00:15, 13.14it/s]\n",
      "202it [00:15, 12.95it/s]\n",
      "202it [00:15, 12.76it/s]\n",
      "202it [00:20,  9.70it/s]\n",
      "202it [00:23,  8.72it/s]\n",
      "202it [00:15, 13.16it/s]\n",
      "202it [00:18, 11.21it/s]\n",
      "202it [00:17, 11.35it/s]\n",
      "202it [00:19, 10.49it/s]\n",
      "202it [00:25,  7.95it/s]\n",
      "202it [00:29,  6.90it/s]\n",
      "202it [00:18, 10.81it/s]\n",
      "202it [00:21,  9.53it/s]\n",
      "202it [00:21,  9.45it/s]\n",
      "202it [00:21,  9.30it/s]\n",
      "202it [00:30,  6.70it/s]\n",
      "202it [00:34,  5.88it/s]\n",
      "202it [00:20,  9.63it/s]\n",
      "202it [00:23,  8.64it/s]\n",
      "202it [00:23,  8.66it/s]\n",
      "202it [00:24,  8.18it/s]\n",
      "202it [00:35,  5.65it/s]\n",
      "202it [00:40,  5.00it/s]\n",
      "202it [00:23,  8.45it/s]\n",
      "202it [00:27,  7.42it/s]\n",
      "202it [00:27,  7.37it/s]\n",
      "202it [00:27,  7.32it/s]\n",
      "202it [00:42,  4.80it/s]\n",
      "202it [00:48,  4.15it/s]\n",
      "202it [00:27,  7.30it/s]\n",
      "202it [00:30,  6.66it/s]\n",
      "202it [00:32,  6.14it/s]\n",
      "202it [00:31,  6.31it/s]\n",
      "202it [00:49,  4.09it/s]\n",
      "202it [00:59,  3.42it/s]\n",
      "202it [00:32,  6.18it/s]\n",
      "202it [00:35,  5.63it/s]\n",
      "202it [00:36,  5.60it/s]\n",
      "202it [00:36,  5.57it/s]\n",
      "202it [00:56,  3.58it/s]\n",
      "202it [01:06,  3.06it/s]\n",
      "202it [00:36,  5.59it/s]\n",
      "202it [00:39,  5.08it/s]\n",
      "202it [00:38,  5.18it/s]\n",
      "202it [00:39,  5.13it/s]\n",
      "202it [01:01,  3.26it/s]\n",
      "202it [01:13,  2.75it/s]\n",
      "202it [00:39,  5.16it/s]\n",
      "202it [00:42,  4.73it/s]\n",
      "202it [00:42,  4.71it/s]\n",
      "202it [00:42,  4.79it/s]\n",
      "202it [01:08,  2.97it/s]\n",
      "202it [01:20,  2.50it/s]\n",
      "202it [00:42,  4.74it/s]\n",
      "202it [00:43,  4.61it/s]\n",
      "202it [00:43,  4.67it/s]\n",
      "202it [00:43,  4.70it/s]\n",
      "202it [01:13,  2.73it/s]\n",
      "202it [01:29,  2.27it/s]\n",
      "202it [00:45,  4.44it/s]\n",
      "202it [00:50,  4.03it/s]\n",
      "202it [00:50,  4.02it/s]\n",
      "202it [00:47,  4.23it/s]\n",
      "202it [01:19,  2.54it/s]\n",
      "202it [01:34,  2.14it/s]\n",
      "202it [00:48,  4.16it/s]\n",
      "202it [00:53,  3.75it/s]\n",
      "202it [00:50,  3.97it/s]\n",
      "202it [00:48,  4.16it/s]\n",
      "202it [01:20,  2.52it/s]\n",
      "202it [01:35,  2.11it/s]\n",
      "202it [00:47,  4.24it/s]\n",
      "202it [00:53,  3.79it/s]\n",
      "202it [00:54,  3.74it/s]\n",
      "202it [00:49,  4.06it/s]\n",
      "202it [01:24,  2.40it/s]\n",
      "202it [01:41,  1.99it/s]\n",
      "202it [00:50,  4.00it/s]\n",
      "202it [01:00,  3.35it/s]\n",
      "202it [01:00,  3.31it/s]\n",
      "202it [00:52,  3.82it/s]\n",
      "202it [01:29,  2.25it/s]\n",
      "202it [01:48,  1.87it/s]\n",
      "202it [00:53,  3.79it/s]\n",
      "202it [01:01,  3.29it/s]\n",
      "202it [01:02,  3.26it/s]\n",
      "202it [00:59,  3.37it/s]\n",
      "202it [01:41,  1.99it/s]\n",
      "202it [02:02,  1.65it/s]\n",
      "202it [01:00,  3.35it/s]\n",
      "202it [01:12,  2.80it/s]\n",
      "202it [01:12,  2.80it/s]\n",
      "202it [01:02,  3.21it/s]\n",
      "202it [01:47,  1.88it/s]\n",
      "202it [02:09,  1.56it/s]\n",
      "202it [01:02,  3.22it/s]\n",
      "202it [01:14,  2.72it/s]\n",
      "202it [01:11,  2.84it/s]\n",
      "202it [01:00,  3.33it/s]\n",
      "202it [01:44,  1.93it/s]\n",
      "202it [02:09,  1.56it/s]\n",
      "202it [01:01,  3.27it/s]\n",
      "202it [01:16,  2.63it/s]\n",
      "202it [01:16,  2.64it/s]\n",
      "202it [01:03,  3.20it/s]\n",
      "202it [01:53,  1.78it/s]\n",
      "202it [02:23,  1.41it/s]\n",
      "202it [01:07,  2.98it/s]\n",
      "202it [01:31,  2.20it/s]\n",
      "202it [01:31,  2.20it/s]\n",
      "202it [01:02,  3.25it/s]\n",
      "202it [01:53,  1.78it/s]\n",
      "202it [02:16,  1.48it/s]\n",
      "202it [01:00,  3.32it/s]\n",
      "202it [01:20,  2.51it/s]\n",
      "202it [01:19,  2.54it/s]\n"
     ]
    }
   ],
   "source": [
    "k_values = list(range(10, 202, 10)) + [None]\n",
    "min_impurity_decrease_values = [0.0, 0.001]\n",
    "max_depth_values = [2, 3, 4, 5]\n",
    "more_params = {}\n",
    "\n",
    "# k_values = [40]\n",
    "# max_depth_values = [2, 3, 4, 5]\n",
    "# min_impurity_decrease_values = [0.0, 0.001, 0.01, 0.1]\n",
    "# more_params = {}\n",
    "\n",
    "for k in k_values:\n",
    "    for min_impurity_decrease in min_impurity_decrease_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            x = do_leave_one_process_out_k(\n",
    "                fit_predict_decision_tree,\n",
    "                df,\n",
    "                df_dec,\n",
    "                process_ids,\n",
    "                k=k,\n",
    "                min_impurity_decrease=min_impurity_decrease,\n",
    "                max_depth=max_depth,\n",
    "                **more_params,\n",
    "            )\n",
    "            pd.DataFrame({\"scores\": x[0], \"counts\": x[1], \"counts2\": x[2]}).to_csv(\n",
    "                TMP_DIR\n",
    "                / TREE_FILENAME_OUT_TEMPLATE.format(\n",
    "                    k=k,\n",
    "                    min_impurity_decrease=min_impurity_decrease,\n",
    "                    max_depth=max_depth,\n",
    "                    **more_params,\n",
    "                ),\n",
    "                sep=\";\",\n",
    "                index=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202/202 [00:15<00:00, 13.44it/s]\n"
     ]
    }
   ],
   "source": [
    "min_impurity_decrease = 0.0\n",
    "max_depth = 5\n",
    "more_params = {}\n",
    "\n",
    "x = do_leave_one_process_out_same_category(\n",
    "    fit_predict_decision_tree,\n",
    "    df,\n",
    "    df_dec,\n",
    "    process_ids,\n",
    "    process_category,\n",
    "    min_impurity_decrease=min_impurity_decrease,\n",
    "    max_depth=max_depth,\n",
    "    **more_params,\n",
    ")\n",
    "pd.DataFrame({\"scores\": x[0], \"counts\": x[1], \"counts2\": x[2]}).to_csv(\n",
    "    TMP_DIR\n",
    "    / TREE_FILENAME_OUT_TEMPLATE_SAME_CATEGORY.format(\n",
    "        min_impurity_decrease=min_impurity_decrease, max_depth=max_depth, **more_params\n",
    "    ),\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bireducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_values = [20, 50, 80, 110, 140, 170, 200]\n",
    "# k_values = [10, 30, 60, 70, 90, 100, 120, 130, 150, 160, 180, 190, None]\n",
    "k_values = list(range(10, 202, 10)) + [None]\n",
    "n_bireducts_values = [1000]\n",
    "candidate_n_attrs_values = [100]\n",
    "allowed_randomness_values = [0.05]\n",
    "max_n_attrs_values = [3]\n",
    "more_params = {}\n",
    "\n",
    "for iteration in range(3):\n",
    "    if iteration < 1:\n",
    "        continue\n",
    "    for k in k_values:\n",
    "        if k < 70:\n",
    "            continue\n",
    "        for n_bireducts in n_bireducts_values:\n",
    "            for candidate_n_attrs in candidate_n_attrs_values:\n",
    "                for allowed_randomness in allowed_randomness_values:\n",
    "                    for max_n_attrs in max_n_attrs_values:\n",
    "                        x = do_leave_one_process_out_k(\n",
    "                            fit_predict_bireducts,\n",
    "                            df,\n",
    "                            df_dec,\n",
    "                            process_ids,\n",
    "                            k=k,\n",
    "                            n_bireducts=n_bireducts,\n",
    "                            candidate_n_attrs=candidate_n_attrs,\n",
    "                            allowed_randomness=allowed_randomness,\n",
    "                            max_n_attrs=max_n_attrs,\n",
    "                            **more_params,\n",
    "                        )\n",
    "                        pd.DataFrame(\n",
    "                            {\"scores\": x[0], \"counts\": x[1], \"counts2\": x[2]}\n",
    "                        ).to_csv(\n",
    "                            TMP_DIR\n",
    "                            / BIREDUCTS_FILENAME_OUT_TEMPLATE.format(\n",
    "                                k=k,\n",
    "                                n_bireducts=n_bireducts,\n",
    "                                candidate_n_attrs=candidate_n_attrs,\n",
    "                                allowed_randomness=allowed_randomness,\n",
    "                                max_n_attrs=max_n_attrs,\n",
    "                                iteration=iteration,\n",
    "                                **more_params,\n",
    "                            ),\n",
    "                            sep=\";\",\n",
    "                            index=False,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bireducts = 1000\n",
    "candidate_n_attrs = 100\n",
    "allowed_randomness = 0.05\n",
    "max_n_attrs = 3\n",
    "iteration = 1\n",
    "x = do_leave_one_process_out_same_category(\n",
    "    fit_predict_bireducts,\n",
    "    df,\n",
    "    df_dec,\n",
    "    process_ids,\n",
    "    process_category,\n",
    "    n_bireducts=n_bireducts,\n",
    "    candidate_n_attrs=candidate_n_attrs,\n",
    "    allowed_randomness=allowed_randomness,\n",
    "    max_n_attrs=max_n_attrs,\n",
    "    **more_params,\n",
    ")\n",
    "pd.DataFrame({\"scores\": x[0], \"counts\": x[1], \"counts2\": x[2]}).to_csv(\n",
    "    TMP_DIR\n",
    "    / BIREDUCTS_FILENAME_OUT_TEMPLATE_SAME_CATEGORY.format(\n",
    "        n_bireducts=n_bireducts,\n",
    "        candidate_n_attrs=candidate_n_attrs,\n",
    "        allowed_randomness=allowed_randomness,\n",
    "        max_n_attrs=max_n_attrs,\n",
    "        iteration=iteration,\n",
    "        **more_params,\n",
    "    ),\n",
    "    sep=\";\",\n",
    "    index=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb353cb8b9a4ebafc945efb5c6e15e592d37ff0fcf51b748ec6c5769c1eaa51b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
